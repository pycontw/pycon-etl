[core]
# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 256

# The number of task instances allowed to run concurrently by the scheduler
max_active_tasks_per_dag = 64

# Whether to load the examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = False

# Secret key to save connection passwords in the db
fernet_key = $FERNET_KEY

# How long before timing out a python file import
dagbag_import_timeout = 600

auth_manager = airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

[dag_processor]
# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 600
# We don't really need it based on how we deploy dags.
# But set it to a large enough 60 * 24 (1 day) to avoid high CPU usage
min_file_process_interval = 1440

[database]
# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
sql_alchemy_conn = sqlite:////opt/airflow/sqlite/airflow.db
external_db_managers = airflow.providers.fab.auth_manager.models.db.FABDBManager


[api]
# Number of workers to run the Gunicorn web server
# WARNING:: DO NOT increase this number. Due to our limited resources, increasing it to 2 breaks API server
workers = 1

# Number of seconds the gunicorn webserver waits before timing out on a worker
worker_timeout = 600

# Expose the configuration file in the web server
expose_config = True

# TODO: move it to env var
# Secret key used to run your flask app
# It should be as random as possible
secret_key = l\xba,\xc3\x023\xca\x04\xdb\xf2\xf7\xfa\xb8#\xee>

[api_auth]
jwt_secret = $JWT_SECRET

[fab]
auth_backends = airflow.providers.fab.auth_manager.api.auth.backend.session

# TODO: check whether we need it. I don't think we're using celery
[celery]
# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 32

# The maximum and minimum concurrency that will be used when starting workers with the
# ``airflow celery worker`` command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Example: worker_autoscale = 16,12
worker_autoscale = 32,12

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://redis:6379/1

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+postgresql://airflow:airflow@postgres/airflow


[scheduler]
# TODO: check whether it exists
enable_health_check = True


[kubernetes]
# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis.
# See:
# https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
# Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely
# for kubernetes api responses, which will cause the scheduler to hang.
# The timeout is specified as [connect timeout, read timeout]
kube_client_request_args = {{"_request_timeout" : [60,60] }}

# ref: https://airflow.apache.org/docs/apache-airflow/1.10.1/security.html#setting-up-google-authentication
[google]
client_id = <check the doc above>
client_secret = <check the doc above>
oauth_callback_route = /oauth2callback
domain = localhost,pycon.tw
prompt = select_account
