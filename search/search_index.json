{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyCon TW ETL","text":"<p>Using Airflow to implement our ETL pipelines.</p> <ul> <li>PyCon TW ETL<ul> <li>Prerequisites</li> <li>Installation<ul> <li>1. Create a Virtual Environment with Dependencies Installed</li> <li>2. Activate the Virtual Environment</li> <li>3. Deactivate the Virtual Environment</li> </ul> </li> <li>Configuration<ul> <li>BigQuery (Optional)</li> </ul> </li> <li>Running the Project<ul> <li>Local Development with uv</li> <li>Local Development with docker-compose<ul> <li>Use images from Artifacts</li> </ul> </li> </ul> </li> <li>Contact</li> </ul> </li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10</li> <li>Docker</li> <li>Git</li> <li>uv</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>We use uv to manage dependencies and virtual environment.</p> <p>Below are the steps to create a virtual environment using uv:</p>"},{"location":"#1-create-a-virtual-environment-with-dependencies-installed","title":"1. Create a Virtual Environment with Dependencies Installed","text":"<p>To create a virtual environment, run the following command:</p> <pre><code>uv sync\n</code></pre> <p>By default, uv sets up the virtual environment in <code>.venv</code></p>"},{"location":"#2-activate-the-virtual-environment","title":"2. Activate the Virtual Environment","text":"<p>After creating the virtual environment, activate it using the following command:</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"#3-deactivate-the-virtual-environment","title":"3. Deactivate the Virtual Environment","text":"<p>When you're done working in the virtual environment, you can deactivate it with:</p> <pre><code>deactivate\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<ol> <li>For development or testing, run <code>cp .env.template .env.staging</code>. For production, run <code>cp .env.template .env.production</code>.</li> <li>Follow the instructions in <code>.env.&lt;staging|production&gt;</code> and fill in your secrets. If you are running the staging instance for development as a sandbox and do not need to access any specific third-party services, leaving <code>.env.staging</code> as-is should be fine.</li> </ol> <p>Contact the maintainer if you don't have these secrets.</p> <p>\u26a0 WARNING: About .env Please do not use the .env file for local development, as it might affect the production tables.</p>"},{"location":"#bigquery-optional","title":"BigQuery (Optional)","text":"<ul> <li>Set up the Authentication for GCP: https://googleapis.dev/python/google-api-core/latest/auth.html</li> <li>After running <code>gcloud auth application-default login</code>, you will get a credentials.json file located at <code>$HOME/.config/gcloud/application_default_credentials.json</code>.</li> <li>Run <code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/keyfile.json\"</code> if you have it.</li> <li><code>service-account.json</code>: Please contact @david30907d via email or Discord. You do not need this json file if you are running the sandbox staging instance for development.</li> </ul>"},{"location":"#running-the-project","title":"Running the Project","text":"<p>If you are a developer \ud83d\udc68\u200d\ud83d\udcbb, please check the Contributing Guide.</p> <p>If you are a maintainer \ud83d\udc68\u200d\ud83d\udd27, please check the Maintenance Guide.</p>"},{"location":"#local-development-with-uv","title":"Local Development with uv","text":"<pre><code># point the database to local \"sqlite/airflow.db\"\n# Run \"uv run airflow db migrate\" if the file does not exist\nexport AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=`pwd`/sqlite/airflow.db\n\n# point the airflow home to current directory\nexport AIRFLOW_HOME=`pwd`\n\n# Run standalone airflow\n# Note that there may be slight differences between using this command and running through docker compose\n# However, the difference should not be noticeable in most cases.\nuv run airflow standalone\n</code></pre>"},{"location":"#local-development-with-docker-compose","title":"Local Development with docker-compose","text":"<pre><code># Build the local dev/test image\nmake build-dev\n\n# Start dev/test services\nmake deploy-dev\n\n# Stop dev/test services\nmake down-dev\n</code></pre> <p>The difference between production and dev/test compose files is that the dev/test compose file uses a locally built image, while the production compose file uses the image from Docker Hub.</p>"},{"location":"#use-images-from-artifacts","title":"Use images from Artifacts","text":"<p>If you are an authorized maintainer, you can pull the image from the [GCP Artifact Registry].</p> <p>Docker client must be configured to use the [GCP Artifact Registry].</p> <pre><code>gcloud auth configure-docker asia-east1-docker.pkg.dev\n</code></pre> <p>Then, pull the image:</p> <pre><code>docker pull asia-east1-docker.pkg.dev/pycontw-225217/data-team/pycon-etl:{tag}\n</code></pre> <p>Available tags:</p> <ul> <li><code>cache</code>: cache the image for faster deployment</li> <li><code>test</code>: for testing purposes, including the test dependencies</li> <li><code>staging</code>: when pushing to the staging environment</li> <li><code>latest</code>: when pushing to the production environment</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>PyCon TW Volunteer Data Team - Discord</p>"},{"location":"CONTRIBUTING/","title":"Contributing Guide","text":""},{"location":"CONTRIBUTING/#how-to-contribute-to-this-project","title":"How to Contribute to this Project","text":""},{"location":"CONTRIBUTING/#1-clone-this-repository","title":"1. Clone this repository:","text":"<pre><code>git clone https://github.com/pycontw/pycon-etl\n</code></pre>"},{"location":"CONTRIBUTING/#2-create-a-new-branch","title":"2. Create a new branch:","text":"<p>Please checkout your branch from the latest master branch before doing any code change.</p> <pre><code># Checkout to the master branch\ngit checkout master\n\n# Ensure that's you're on the latest master branch\ngit pull origin master\n\n# Create a new branch\ngit checkout -b &lt;branch-name&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#3-make-your-changes","title":"3. Make your changes.","text":"<p>If your task uses an external service, add the connection and variable in the Airflow UI.</p>"},{"location":"CONTRIBUTING/#4-test-your-changes-in-your-local-environment","title":"4. Test your changes in your local environment:","text":"<ul> <li>Ensure that the dag files are loaded successfully.</li> <li>Verify that the tasks run without errors.</li> <li>Confirm that your code is properly formatted and linted. See Convention section for more details.</li> <li>Check that all necessary dependencies are included in the <code>pyproject.toml</code> file.</li> <li>Airflow dependencies are managed by uv.</li> <li>Ensure that all required documentation is provided.</li> </ul>"},{"location":"CONTRIBUTING/#5-push-your-branch","title":"5. Push your branch:","text":"<pre><code>git push origin &lt;branch-name&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#6-create-a-pull-request-pr","title":"6. Create a Pull Request (PR).","text":"<p>If additional steps are required after merging and deploying (e.g., add new connections or variables), please list them in the PR description.</p>"},{"location":"CONTRIBUTING/#7-wait-for-the-review-and-merge","title":"7. Wait for the review and merge.","text":""},{"location":"CONTRIBUTING/#convention","title":"Convention","text":""},{"location":"CONTRIBUTING/#airflow-dags","title":"Airflow Dags","text":"<ul> <li>Please refer to \u300c\u5927\u6578\u64da\u4e4b\u8def\uff1a\u963f\u91cc\u5df4\u5df4\u5927\u6578\u64da\u5be6\u6230\u300d \u8b80\u66f8\u5fc3\u5f97 for naming guidelines.</li> <li>Table name convention:   </li> </ul>"},{"location":"CONTRIBUTING/#code-formatting","title":"Code Formatting","text":"<p>Please run <code>make format</code> to ensure your code is properly formatted before committing; otherwise, the CI will fail.</p>"},{"location":"CONTRIBUTING/#commit-message","title":"Commit Message","text":"<p>It is recommended to use Commitizen.</p>"},{"location":"CONTRIBUTING/#release-management-cicd","title":"Release Management (CI/CD)","text":"<p>We use Python CI and Docker Image CI to ensure our code quality meets specific standards and that Docker images can be published automatically.</p> <p>When a pull request is created, Python CI checks whether the code quality is satisfactory. At the same time, we build a <code>cache</code> image using <code>Dockerfile</code> and a <code>test</code> image with <code>Dockerfile.test</code>, which are then pushed to the GCP Artifact Registry.</p> <p>After a pull request is merged into the <code>master</code> branch, the two image tags mentioned above are created, along with a new <code>staging</code> tag for the image generated from <code>Dockerfile</code>.</p> <p>Once we verify that the <code>staging</code> image functions correctly, we merge the <code>master</code> branch into the <code>prod</code> branch through the following commands.</p> <pre><code>git checkout prod\ngit pull origin prod\n\ngit merge origin/master\n\ngit pull origin prod\n</code></pre> <p>This triggers the Docker Image CI again to update the <code>cache</code>, <code>test</code>, and <code>staging</code> images, as well as to create a <code>latest</code> image that we will later use for deploying to our production instance. See the Deployment Guide for the following steps.</p> <pre><code>---\nconfig:\n  theme: 'base'\n  gitGraph:\n    mainBranchName: 'prod'\n    tagLabelFontSize: '25px'\n    branchLabelFontSize: '20px'\n---\n      gitGraph\n        commit id:\"latest features\" tag:\"latest\"\n        branch master\n        commit id:\"staging features\" tag:\"staging\"\n        checkout prod\n        commit id:\"prod config\"\n        checkout master\n        branch feature-1\n        commit id: \"new features\" tag:\"cache\" tag:\"test\"</code></pre>"},{"location":"DEPLOYMENT/","title":"Deployment Guide","text":""},{"location":"DEPLOYMENT/#start-deploying","title":"Start Deploying","text":""},{"location":"DEPLOYMENT/#1-login-to-the-data-teams-gce-server","title":"1. Login to the data team's GCE server","text":"<pre><code>gcloud compute ssh --zone \"asia-east1-b\" \"data-team\" --project \"pycontw-225217\"\n</code></pre> <ul> <li>Location of the Services:<ul> <li>ETL (airflow): <code>/srv/pycon-etl</code></li> <li>Metabase: <code>/mnt/disks/data-team-additional-disk/pycontw-infra-scripts/data_team/metabase_server</code></li> </ul> </li> </ul>"},{"location":"DEPLOYMENT/#2-pull-the-latest-codebase-and-image-to-this-server","title":"2. Pull the latest codebase and image to this server","text":"<pre><code>git checkout prod\ngit pull origin prod\n\ndocker pull asia-east1-docker.pkg.dev/pycontw-225217/data-team/pycon-etl:latest\n</code></pre>"},{"location":"DEPLOYMENT/#3-add-credentials-to-the-envproduction-file-only-needs-to-be-done-once","title":"3. Add credentials to the <code>.env.production</code> file (only needs to be done once).","text":""},{"location":"DEPLOYMENT/#4-restart-the-services","title":"4. Restart the services:","text":"<pre><code># Start production services\ndocker-compose -f ./docker-compose.yml up\n\n# Stop production services\n# docker-compose -f ./docker-compose.yml down\n</code></pre>"},{"location":"DEPLOYMENT/#5-check-whether-the-services-are-up","title":"5. Check whether the services are up","text":"<pre><code># For Airflow, the following services should be included: \n# * airflow-api-server\n# * airflow-dag-processor\n# * airflow-scheduler\n# * airflow-triggerer\ndocker ps\n\n# Check the resource usage if needed\ndocker stats\n</code></pre>"},{"location":"DEPLOYMENT/#6-login-to-the-service","title":"6. Login to the service","text":"<p>For security reasons, our Airflow instance is not publicly accessible. You will need an authorized GCP account to perform port forwarding for the webserver and an authorized Airflow account to access it.</p>"},{"location":"MAINTENANCE/","title":"Maintenance Guide","text":""},{"location":"MAINTENANCE/#disk-space","title":"Disk Space","text":"<p>Currently, the disk space is limited, so please check the disk space before running any ETL jobs.</p> <p>This section will be deprecated if we no longer encounter out-of-disk issues.</p>"},{"location":"MAINTENANCE/#1-find-the-largest-folders","title":"1. Find the largest folders:","text":"<pre><code>du -a /var/lib/docker/overlay2 | sort -n -r | head -n 20\n</code></pre>"},{"location":"MAINTENANCE/#2-show-the-folder-size","title":"2. Show the folder size:","text":"<pre><code>du -hs \n</code></pre>"},{"location":"MAINTENANCE/#3-delete-the-large-folders-identified","title":"3. Delete the large folders identified.","text":""},{"location":"MAINTENANCE/#4-check-disk-space","title":"4. Check disk space:","text":"<pre><code>df -h\n</code></pre>"},{"location":"MAINTENANCE/#token-expiration","title":"Token Expiration","text":"<p>Some API tokens might expire, so please check them regularly.</p>"},{"location":"MAINTENANCE/#year-to-year-jobs","title":"Year-to-Year Jobs","text":"<p>Please refer to Dev Data Team - Year to Year Jobs - HackMD for more details.</p>"}]}